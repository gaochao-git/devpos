#!/usr/bin/env python3
"""
ğŸ”¥ é˜²ç«å¢™æµé‡ä¸ä¸šåŠ¡å“åº”å…³è”åˆ†æè„šæœ¬ (ç®€åŒ–ç‰ˆ)

é€‚ç”¨äºå®šæ—¶ä»»åŠ¡è°ƒåº¦ï¼š
- æ£€æµ‹æŒ‡å®šæ—¶é—´çª—å£å†…æµé‡æ˜¯å¦æŒç»­å‡é«˜
- å¦‚æœæµé‡æŒç»­å‡é«˜ï¼Œæ£€æŸ¥ä¸šåŠ¡å“åº”è€—æ—¶
- åˆ¤æ–­æµé‡å¢åŠ æ˜¯å¦å½±å“ä¸šåŠ¡å“åº”

ç”¨æ³•:
  python firewall_simple_monitor.py <æœºæˆ¿> <IP> <æ—¶é—´çª—å£ç§’æ•°> [debug=on|off]
  
ç¤ºä¾‹:
  python firewall_simple_monitor.py IDC1 192.168.1.1 300
  python firewall_simple_monitor.py IDC1 192.168.1.1 300 debug=on
  python firewall_simple_monitor.py IDC2 10.0.0.1 600 debug=off
"""

import sys
from datetime import datetime, timedelta
import requests
from typing import Dict, List, Optional

# DeepSeek API é…ç½®
DEEPSEEK_CONFIG = {
    "api_key": "sk-490738f8ce8f4a36bcc0bfb165270008",
    "api_base": "https://api.deepseek.com/v1",
    "model": "deepseek-chat",
    "timeout": 30
}

# é˜²ç«å¢™é…ç½®
FIREWALL_CONFIG = {
    "max_bandwidth_gbps": 40,  # é˜²ç«å¢™æœ€å¤§å¸¦å®½ 40Gbps
    "increase_threshold": 0.6,  # 60%çš„æ•°æ®ç‚¹å¢åŠ æ‰åˆ¤å®šä¸ºæŒç»­å‡é«˜
    "response_degradation_threshold": 1.3,  # å“åº”æ—¶é—´å¢åŠ 30%åˆ¤å®šä¸ºæ¶åŒ–
    "traffic_baseline_bps": 5 * 1000 * 1000, # æµé‡åŸºçº¿ 10Mbpsï¼Œä½äºæ­¤å€¼ä¸é‡ç‚¹åˆ†æ
    "db_response_baseline_ms": 4  # æ•°æ®åº“å“åº”åŸºçº¿ 4msï¼Œä½äºæ­¤å€¼ä¸é‡ç‚¹åˆ†æ
}

# é˜²ç«å¢™æµé‡æŒ‡æ ‡é…ç½®
TRAFFIC_METRICS = {
    "inbound_traffic": "net.if.in[eth0]",      # å…¥æµé‡
    "outbound_traffic": "net.if.out[eth0]",    # å‡ºæµé‡
}

# ä¸šåŠ¡å“åº”æŒ‡æ ‡é…ç½®  
BUSINESS_METRICS = {
    "database_response": "db.commit.rt"        # æ•°æ®åº“æäº¤å“åº”æ—¶é—´
}

def log_message(message, level="INFO"):
    """è®°å½•æ—¥å¿—æ¶ˆæ¯"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f"[{timestamp}] [{level}] {message}")

def call_deepseek_api(prompt, max_retries=2):
    """è°ƒç”¨DeepSeek APIè¿›è¡Œæ™ºèƒ½åˆ†æ"""
    try:
        config = DEEPSEEK_CONFIG
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {config['api_key']}"
        }
        
        data = {
            "model": config["model"],
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ç½‘ç»œè¿ç»´ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºç»™å‡ºçš„æ•°æ®ï¼Œåˆ†æåœ¨ç½‘ç»œæµé‡é«˜å¢é•¿çš„æ—¶é—´åŒºé—´å†…ï¼Œæ•°æ®åº“çš„å“åº”è€—æ—¶æ˜¯å¦ä¹ŸåŒæ­¥å¢åŠ ã€‚è¯·ç›´æ¥é™ˆè¿°åˆ†æç»“è®ºï¼Œä¸è¦æä¾›ä»»ä½•å»ºè®®æˆ–æ¨æµ‹æ ¹å› ã€‚"
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            "temperature": 0.7,
            "max_tokens": 800
        }
        
        for attempt in range(max_retries):
            try:
                response = requests.post(
                    config["api_base"] + "/chat/completions",
                    headers=headers,
                    json=data,
                    timeout=config["timeout"]
                )
                
                if response.status_code == 200:
                    result = response.json()
                    if "choices" in result and len(result["choices"]) > 0:
                        return result["choices"][0]["message"]["content"]
                        
            except requests.exceptions.RequestException:
                if attempt < max_retries - 1:
                    continue
                    
    except Exception as e:
        log_message(f"APIè°ƒç”¨é”™è¯¯: {e}", "ERROR")
        
    return None

def get_metrics_data(host_ip: str, metrics: Dict[str, str], time_from: datetime, time_till: datetime) -> Dict[str, List]:
    """è·å–æŒ‡æ ‡æ•°æ®"""
    try:
        from zabbix_api_util import get_zabbix_metrics
        
        time_from_ts = int(time_from.timestamp())
        time_till_ts = int(time_till.timestamp())
        
        print(f"\nğŸ” è°ƒè¯•ï¼šè·å–æŒ‡æ ‡æ•°æ®")
        print(f"ä¸»æœºIP: {host_ip}")
        print(f"æ—¶é—´èŒƒå›´: {time_from.strftime('%Y-%m-%d %H:%M:%S')} ~ {time_till.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æ—¶é—´æˆ³: {time_from_ts} ~ {time_till_ts}")
        
        metrics_data = {}
        
        for metric_name, metric_key in metrics.items():
            print(f"\nğŸ“Š è·å–æŒ‡æ ‡: {metric_name} ({metric_key})")
            
            result = get_zabbix_metrics(
                host_ip=host_ip,
                metric_name=metric_key,
                time_from=time_from_ts,
                time_till=time_till_ts,
                match_type='search',
                limit=1000
            )
            
            print(f"APIè¿”å›çŠ¶æ€: {result.get('status', 'unknown')}")
            print(f"APIè¿”å›æ¶ˆæ¯: {result.get('message', 'no message')}")
            
            if result['status'] == 'success' and result['data']:
                print(f"è·å–åˆ°æ•°æ®ç‚¹æ•°é‡: {len(result['data'])}")
                
                # æ˜¾ç¤ºå‰5ä¸ªåŸå§‹æ•°æ®ç‚¹
                print(f"\nğŸ“‹ å‰5ä¸ªåŸå§‹æ•°æ®ç‚¹:")
                for i, item in enumerate(result['data'][:5]):
                    timestamp = int(item['clock'])
                    time_str = datetime.fromtimestamp(timestamp).strftime('%H:%M:%S')
                    print(f"  [{i+1}] æ—¶é—´: {time_str}, æ—¶é—´æˆ³: {timestamp}, åŸå§‹å€¼: {item['value']}, ç±»å‹: {type(item['value'])}")
                
                # å¤„ç†æ•°æ®
                data = []
                raw_values = []
                
                for item in result['data']:
                    try:
                        raw_value = float(item['value'])
                        raw_values.append(raw_value)
                        data.append({
                            'time': datetime.fromtimestamp(int(item['clock'])),
                            'value': raw_value  # ç›´æ¥ä½¿ç”¨åŸå§‹å€¼ï¼Œå‡è®¾æ˜¯bps
                        })
                    except (ValueError, KeyError) as e:
                        print(f"  âš ï¸ æ•°æ®è½¬æ¢é”™è¯¯: {e}, é¡¹ç›®: {item}")
                        continue
                
                if data:
                    min_val = min(raw_values)
                    max_val = max(raw_values)
                    avg_val = sum(raw_values) / len(raw_values)
                    
                    print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
                    print(f"  æœ‰æ•ˆæ•°æ®ç‚¹: {len(data)}")
                    print(f"  æ•°å€¼èŒƒå›´: {min_val:.2f} ~ {max_val:.2f}")
                    print(f"  å¹³å‡å€¼: {avg_val:.2f}")
                    print(f"  å•ä½æ¨æµ‹: {'å¯èƒ½æ˜¯bps' if max_val > 1000 else 'å¯èƒ½æ˜¯Kbpsæˆ–å…¶ä»–'}")
                    
                    # æ ¼å¼åŒ–æ˜¾ç¤ºç¤ºä¾‹
                    print(f"\nğŸ¨ æ ¼å¼åŒ–æ˜¾ç¤ºç¤ºä¾‹:")
                    print(f"  æœ€å°å€¼: {format_traffic_rate(min_val, is_bits=True)} (å½“ä½œbps)")
                    print(f"  æœ€å¤§å€¼: {format_traffic_rate(max_val, is_bits=True)} (å½“ä½œbps)")
                    print(f"  å¹³å‡å€¼: {format_traffic_rate(avg_val, is_bits=True)} (å½“ä½œbps)")
                    
                    if max_val > 0:
                        print(f"\nğŸ”„ å¦‚æœå½“ä½œbytes/sä¼šæ˜¯:")
                        print(f"  æœ€å¤§å€¼: {format_traffic_rate(max_val, is_bits=False)} (å½“ä½œbytes/s)")
                        print(f"  å¹³å‡å€¼: {format_traffic_rate(avg_val, is_bits=False)} (å½“ä½œbytes/s)")
                
                log_message(f"æŒ‡æ ‡ {metric_name} è·å–æˆåŠŸ, æ•°æ®ç‚¹: {len(data)}, èŒƒå›´: {min_val:.1f} ~ {max_val:.1f}")
                
                metrics_data[metric_name] = sorted(data, key=lambda x: x['time'])
            else:
                print(f"âŒ è·å–æ•°æ®å¤±è´¥æˆ–æ— æ•°æ®")
                if 'error' in result:
                    print(f"é”™è¯¯è¯¦æƒ…: {result['error']}")
                metrics_data[metric_name] = []
        
        print(f"\nâœ… æ•°æ®è·å–å®Œæˆï¼Œå…±è·å– {len(metrics_data)} ä¸ªæŒ‡æ ‡")
        return metrics_data
        
    except Exception as e:
        print(f"âŒ è·å–æŒ‡æ ‡æ•°æ®å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        log_message(f"è·å–æŒ‡æ ‡æ•°æ®å¤±è´¥: {e}", "ERROR")
        return {}

def format_traffic_rate(bits_or_bytes_per_second: float, is_bits: bool = False) -> str:
    """æ ¹æ®æµé‡å¤§å°åŠ¨æ€æ ¼å¼åŒ–æ˜¾ç¤ºå•ä½
    
    Args:
        bits_or_bytes_per_second: æµé‡å€¼
        is_bits: Trueè¡¨ç¤ºè¾“å…¥å·²ç»æ˜¯bits/sï¼ŒFalseè¡¨ç¤ºæ˜¯bytes/séœ€è¦è½¬æ¢
    """
    if bits_or_bytes_per_second == 0:
        return "0 B/s"
    
    # å¦‚æœä¸æ˜¯bitsï¼Œåˆ™è½¬æ¢ä¸ºbits/second
    if is_bits:
        bits_per_second = bits_or_bytes_per_second
    else:
        bits_per_second = bits_or_bytes_per_second * 8
    
    # ä½¿ç”¨åè¿›åˆ¶è¿›åˆ¶ï¼ˆ1000ï¼‰
    units = [
        (1000**4, "Tbps"),
        (1000**3, "Gbps"), 
        (1000**2, "Mbps"),
        (1000**1, "Kbps"),
        (1, "bps")
    ]
    
    for threshold, unit in units:
        if bits_per_second >= threshold:
            value = bits_per_second / threshold
            if value >= 100:
                return f"{value:.1f} {unit}"
            elif value >= 10:
                return f"{value:.1f} {unit}"
            else:
                return f"{value:.2f} {unit}"
    
    return f"{bits_per_second:.2f} bps"

def bytes_to_gbps(bits_or_bytes_per_second: float, is_bits: bool = False) -> float:
    """å°†æµé‡å€¼è½¬æ¢ä¸ºGbpsï¼ˆä½¿ç”¨åè¿›åˆ¶ï¼‰
    
    Args:
        bits_or_bytes_per_second: æµé‡å€¼
        is_bits: Trueè¡¨ç¤ºè¾“å…¥å·²ç»æ˜¯bits/sï¼ŒFalseè¡¨ç¤ºæ˜¯bytes/séœ€è¦è½¬æ¢
    """
    if is_bits:
        return bits_or_bytes_per_second / (1000 ** 3)
    else:
        return (bits_or_bytes_per_second * 8) / (1000 ** 3)

def identify_traffic_increase_intervals(data: List[Dict]) -> List[Dict]:
    """è¯†åˆ«æµé‡é€’å¢åŒºé—´"""
    if not data or len(data) < 3:
        return []
    
    increase_intervals = []
    current_interval = None
    
    for i in range(1, len(data)):
        prev_value = data[i-1]['value']
        curr_value = data[i]['value']
        
        if curr_value > prev_value:  # æµé‡é€’å¢
            if current_interval is None:
                # å¼€å§‹æ–°çš„é€’å¢åŒºé—´
                current_interval = {
                    'start_time': data[i-1]['time'],
                    'end_time': data[i]['time'],
                    'start_value': prev_value,
                    'end_value': curr_value,
                    'peak_value': curr_value,
                    'data_points': [data[i-1], data[i]]
                }
            else:
                # å»¶ç»­å½“å‰é€’å¢åŒºé—´
                current_interval['end_time'] = data[i]['time']
                current_interval['end_value'] = curr_value
                current_interval['peak_value'] = max(current_interval['peak_value'], curr_value)
                current_interval['data_points'].append(data[i])
        else:
            # æµé‡ä¸å†é€’å¢ï¼Œç»“æŸå½“å‰åŒºé—´
            if current_interval is not None:
                # è®¡ç®—é€’å¢å¹…åº¦
                increase_ratio = (current_interval['end_value'] - current_interval['start_value']) / current_interval['start_value'] if current_interval['start_value'] > 0 else 0
                current_interval['increase_ratio'] = increase_ratio
                current_interval['duration_seconds'] = (current_interval['end_time'] - current_interval['start_time']).total_seconds()
                
                # åªä¿ç•™æœ‰æ„ä¹‰çš„é€’å¢åŒºé—´ï¼ˆå¢é•¿è¶…è¿‡10%æˆ–æŒç»­æ—¶é—´è¶…è¿‡30ç§’ï¼‰
                if increase_ratio > 0.1 or current_interval['duration_seconds'] > 30:
                    increase_intervals.append(current_interval)
                
                current_interval = None
    
    # å¤„ç†æœ€åä¸€ä¸ªåŒºé—´
    if current_interval is not None:
        increase_ratio = (current_interval['end_value'] - current_interval['start_value']) / current_interval['start_value'] if current_interval['start_value'] > 0 else 0
        current_interval['increase_ratio'] = increase_ratio
        current_interval['duration_seconds'] = (current_interval['end_time'] - current_interval['start_time']).total_seconds()
        
        if increase_ratio > 0.1 or current_interval['duration_seconds'] > 30:
            increase_intervals.append(current_interval)
    
    return increase_intervals

def analyze_traffic_with_intervals(data: List[Dict]) -> Dict:
    """åˆ†ææµé‡è¶‹åŠ¿ï¼ŒåŒ…å«é€’å¢åŒºé—´ä¿¡æ¯"""
    if not data or len(data) < 3:
        return {
            "is_increasing": False,
            "avg_gbps": 0,
            "max_gbps": 0,
            "max_formatted": "0 B/s",
            "avg_formatted": "0 B/s",
            "trend": "insufficient_data",
            "utilization_percent": 0,
            "data_points": len(data) if data else 0,
            "increase_intervals": []
        }
    
    values = [item['value'] for item in data]
    gbps_values = [bytes_to_gbps(v, is_bits=True) for v in values]
    
    avg_gbps = sum(gbps_values) / len(gbps_values)
    max_gbps = max(gbps_values)
    avg_bytes = sum(values) / len(values)
    max_bytes = max(values)
    
    # è®¡ç®—åˆ©ç”¨ç‡
    max_bandwidth = FIREWALL_CONFIG["max_bandwidth_gbps"]
    utilization_percent = (max_gbps / max_bandwidth) * 100

    # å¦‚æœå³°å€¼æµé‡ä½äºåŸºçº¿ï¼Œåˆ™è®¤ä¸ºæµé‡æ­£å¸¸ï¼Œä¸è¿›è¡Œæ·±å…¥çš„è¶‹åŠ¿åˆ†æ
    traffic_baseline = FIREWALL_CONFIG.get("traffic_baseline_bps", 0)
    if max_bytes < traffic_baseline:
        return {
            "is_increasing": False,
            "avg_gbps": round(avg_gbps, 2),
            "max_gbps": round(max_gbps, 2),
            "max_formatted": format_traffic_rate(max_bytes, is_bits=True),
            "avg_formatted": format_traffic_rate(avg_bytes, is_bits=True),
            "trend": "normal",
            "utilization_percent": round(utilization_percent, 1),
            "data_points": len(values),
            "increase_ratio": 0,
            "increase_intervals": []
        }
    
    # è¯†åˆ«é€’å¢åŒºé—´
    increase_intervals = identify_traffic_increase_intervals(data)
    
    # æ•´ä½“è¶‹åŠ¿åˆ†æ
    is_increasing = False
    trend = "stable"
    increase_ratio = 0
    
    if len(values) >= 3:
        increasing_count = 0
        for i in range(1, len(gbps_values)):
            if gbps_values[i] > gbps_values[i-1]:
                increasing_count += 1
        
        increase_ratio = increasing_count / (len(gbps_values) - 1)
        if increase_ratio >= FIREWALL_CONFIG["increase_threshold"]:
            is_increasing = True
            trend = "increasing"
        elif increase_ratio <= 0.3:
            trend = "decreasing"
    
    # ä¸ºæ¯ä¸ªé€’å¢åŒºé—´æ·»åŠ æ ¼å¼åŒ–ä¿¡æ¯
    for interval in increase_intervals:
        interval['start_formatted'] = format_traffic_rate(interval['start_value'], is_bits=True)
        interval['end_formatted'] = format_traffic_rate(interval['end_value'], is_bits=True)
        interval['peak_formatted'] = format_traffic_rate(interval['peak_value'], is_bits=True)
        interval['increase_percent'] = interval['increase_ratio'] * 100
    
    return {
        "is_increasing": is_increasing,
        "avg_gbps": round(avg_gbps, 2),
        "max_gbps": round(max_gbps, 2),
        "max_formatted": format_traffic_rate(max_bytes, is_bits=True),
        "avg_formatted": format_traffic_rate(avg_bytes, is_bits=True),
        "trend": trend,
        "utilization_percent": round(utilization_percent, 1),
        "data_points": len(values),
        "increase_ratio": round(increase_ratio, 2) if len(values) >= 3 else 0,
        "increase_intervals": increase_intervals
    }

def analyze_response_trend(data: List[Dict]) -> Dict:
    """åˆ†æå“åº”æ—¶é—´è¶‹åŠ¿"""
    if not data or len(data) < 3:
        return {
            "is_degrading": False,
            "avg_ms": 0,
            "max_ms": 0,
            "trend": "insufficient_data",
            "data_points": len(data) if data else 0
        }
    
    values = [item['value'] for item in data]
    
    avg_ms = sum(values) / len(values)
    max_ms = max(values)
    
    db_baseline = FIREWALL_CONFIG.get("db_response_baseline_ms", 0)
    # åªæœ‰å½“æœ€å¤§å“åº”æ—¶é—´è¶…è¿‡åŸºçº¿æ—¶ï¼Œæ‰è¿›ä¸€æ­¥åˆ†ææ˜¯å¦å­˜åœ¨æ¶åŒ–
    if max_ms < db_baseline:
        return {
            "is_degrading": False,
            "avg_ms": round(avg_ms, 2),
            "max_ms": round(max_ms, 2),
            "trend": "normal",
            "data_points": len(values)
        }

    # å“åº”æ—¶é—´è¶‹åŠ¿åˆ†æ
    is_degrading = False
    trend = "stable"
    
    if len(values) >= 3:
        # æ¯”è¾ƒå‰åŠæ®µå’ŒååŠæ®µçš„å¹³å‡å€¼
        mid_point = len(values) // 2
        earlier_avg = sum(values[:mid_point]) / mid_point
        recent_avg = sum(values[mid_point:]) / (len(values) - mid_point)
        
        if recent_avg > earlier_avg * FIREWALL_CONFIG["response_degradation_threshold"]:
            is_degrading = True
            trend = "degrading"
        elif recent_avg < earlier_avg * 0.8:
            trend = "improving"
    
    return {
        "is_degrading": is_degrading,
        "avg_ms": round(avg_ms, 2),
        "max_ms": round(max_ms, 2),
        "trend": trend,
        "data_points": len(values)
    }

def get_change_intensity_indicator(increase_percent: float) -> str:
    """æ ¹æ®å¢é•¿ç™¾åˆ†æ¯”è¿”å›å˜åŒ–å‰§çƒˆç¨‹åº¦æ ‡è¯†ç¬¦"""
    if increase_percent < 25:
        return "*"
    elif increase_percent < 50:
        return "**"
    elif increase_percent < 100:
        return "***"
    elif increase_percent < 200:
        return "****"
    elif increase_percent < 500:
        return "*****"
    else:
        return "******"

def generate_interval_analysis_prompt(datacenter: str, host_ip: str, time_window: int, critical_interval: Dict, all_intervals: List[Dict]) -> str:
    """ç”ŸæˆåŒºé—´åˆ†æçš„AIæç¤ºè¯"""
    
    # åˆ†åˆ«æ”¶é›†å…¥ç«™æµé‡ã€å‡ºç«™æµé‡çš„æ•°æ®
    inbound_intervals = [i for i in all_intervals if i['traffic_type'] == 'inbound']
    outbound_intervals = [i for i in all_intervals if i['traffic_type'] == 'outbound']
    
    # å…¥ç«™æµé‡åˆ†æ
    inbound_analysis = ""
    if inbound_intervals:
        max_inbound_increase = max(interval['increase_percent'] for interval in inbound_intervals)
        max_inbound_interval = max(inbound_intervals, key=lambda x: x['increase_percent'])
        max_inbound_absolute = max(inbound_intervals, key=lambda x: x['peak_value'])
        
        inbound_analysis = f"""ã€å…¥ç«™æµé‡åˆ†æã€‘
æœ€å¤§å¢é•¿: {max_inbound_increase:.1f}%
å‘ç”Ÿæ—¶é—´: {max_inbound_interval['start_time'].strftime('%Y-%m-%d %H:%M:%S')} ~ {max_inbound_interval['end_time'].strftime('%Y-%m-%d %H:%M:%S')}
æµé‡å˜åŒ–: {max_inbound_interval['start_formatted']} -> {max_inbound_interval['end_formatted']}
æœ€å¤§æµé‡å€¼: {max_inbound_absolute['peak_formatted']}
å½±å“åŒºé—´æ•°: {len(inbound_intervals)}ä¸ª"""
    else:
        inbound_analysis = "ã€å…¥ç«™æµé‡åˆ†æã€‘\næ— å…¥ç«™æµé‡å¼‚å¸¸åŒºé—´"
    
    # å‡ºç«™æµé‡åˆ†æ
    outbound_analysis = ""
    if outbound_intervals:
        max_outbound_increase = max(interval['increase_percent'] for interval in outbound_intervals)
        max_outbound_interval = max(outbound_intervals, key=lambda x: x['increase_percent'])
        max_outbound_absolute = max(outbound_intervals, key=lambda x: x['peak_value'])
        
        outbound_analysis = f"""ã€å‡ºç«™æµé‡åˆ†æã€‘
æœ€å¤§å¢é•¿: {max_outbound_increase:.1f}%
å‘ç”Ÿæ—¶é—´: {max_outbound_interval['start_time'].strftime('%Y-%m-%d %H:%M:%S')} ~ {max_outbound_interval['end_time'].strftime('%Y-%m-%d %H:%M:%S')}
æµé‡å˜åŒ–: {max_outbound_interval['start_formatted']} -> {max_outbound_interval['end_formatted']}
æœ€å¤§æµé‡å€¼: {max_outbound_absolute['peak_formatted']}
å½±å“åŒºé—´æ•°: {len(outbound_intervals)}ä¸ª"""
    else:
        outbound_analysis = "ã€å‡ºç«™æµé‡åˆ†æã€‘\næ— å‡ºç«™æµé‡å¼‚å¸¸åŒºé—´"
    
    # æ•°æ®åº“å“åº”è€—æ—¶åˆ†æ
    max_response_degradation = 0
    max_response_interval = None
    min_response_time = float('inf')
    max_response_time = 0
    avg_response_time = 0
    max_response_earlier_avg = 0
    max_response_recent_avg = 0
    
    response_times = []
    for interval in all_intervals:
        business_impact = interval['business_impact']
        response_data = business_impact.get('response_data', [])
        if response_data and len(response_data) >= 3:
            values = [item['value'] for item in response_data]
            response_times.extend(values)
            
            # è®¡ç®—å“åº”æ—¶é—´æ¶åŒ–
            mid_point = len(values) // 2
            earlier_avg = sum(values[:mid_point]) / mid_point if mid_point > 0 else values[0]
            recent_avg = sum(values[mid_point:]) / (len(values) - mid_point)
            degradation_ratio = (recent_avg - earlier_avg) / earlier_avg if earlier_avg > 0 else 0
            
            if degradation_ratio > max_response_degradation:
                max_response_degradation = degradation_ratio
                max_response_interval = interval
                max_response_earlier_avg = earlier_avg
                max_response_recent_avg = recent_avg
    
    if response_times:
        min_response_time = min(response_times)
        max_response_time = max(response_times)
        avg_response_time = sum(response_times) / len(response_times)
    
    db_analysis = f"""ã€æ•°æ®åº“å“åº”è€—æ—¶åˆ†æã€‘
æœ€å¤§å“åº”è€—æ—¶å¢é•¿ç‡: {max_response_degradation*100:.1f}%"""
    
    if max_response_interval:
        db_analysis += f"""
å“åº”å˜åŒ–: {max_response_earlier_avg:.1f}ms -> {max_response_recent_avg:.1f}ms
æ¶åŒ–å‘ç”Ÿæ—¶é—´: {max_response_interval['start_time'].strftime('%Y-%m-%d %H:%M:%S')} ~ {max_response_interval['end_time'].strftime('%Y-%m-%d %H:%M:%S')}
è¯¥æ—¶æ®µå“åº”: å¹³å‡{max_response_interval['business_impact']['analysis']['avg_ms']:.1f}ms, æœ€å¤§{max_response_interval['business_impact']['analysis']['max_ms']:.1f}ms"""
    
    if response_times:
        db_analysis += f"""
æ•´ä½“å“åº”æ—¶é—´: æœ€å°{min_response_time:.1f}ms, æœ€å¤§{max_response_time:.1f}ms, å¹³å‡{avg_response_time:.1f}ms
æ•°æ®ç‚¹æ€»æ•°: {len(response_times)}ä¸ª"""
    
    # ç»Ÿè®¡å½±å“åŒºé—´
    total_intervals = len(all_intervals)
    severe_intervals = len([i for i in all_intervals if i['increase_percent'] >= 500])  # ******çº§åˆ«
    critical_intervals = len([i for i in all_intervals if i['increase_percent'] >= 200])  # *****çº§åˆ«ä»¥ä¸Š
    
    # å½±å“çº§åˆ«åˆ¤å®š
    if severe_intervals >= 3:
        impact_level = "ç´§æ€¥"
    elif critical_intervals >= 5:
        impact_level = "ä¸¥é‡"
    elif total_intervals >= 10:
        impact_level = "ä¸­ç­‰"
    else:
        impact_level = "è½»å¾®"
    
    # æ—¶é—´è·¨åº¦
    first_time = min(interval['start_time'] for interval in all_intervals)
    last_time = max(interval['end_time'] for interval in all_intervals)
    duration_minutes = (last_time - first_time).total_seconds() / 60
    
    prompt = f"""é˜²ç«å¢™æµé‡ä¸æ•°æ®åº“å“åº”å…³è”åˆ†æï¼š

æœºæˆ¿: {datacenter}
é˜²ç«å¢™IP: {host_ip}
åˆ†ææ—¶é—´çª—å£: {time_window}ç§’

ã€ç»¼åˆå½±å“è¯„ä¼°ã€‘
{inbound_analysis}

{outbound_analysis}

{db_analysis}

è¯·åŸºäºä»¥ä¸Šä¸‰ä¸ªç»´åº¦æ•°æ®åˆ†ææµé‡å¢é•¿å¯¹æ•°æ®åº“å“åº”çš„å½±å“ã€‚"""
    
    return prompt

def generate_interval_impact_report(datacenter: str, host_ip: str, time_window: int, impact_intervals: List[Dict], debug_mode: bool):
    """ç”Ÿæˆå—å½±å“åŒºé—´çš„ç»¼åˆåˆ†ææŠ¥å‘Š"""
    
    print(f"\nğŸ”¥ é˜²ç«å¢™æµé‡é€’å¢åŒºé—´å½±å“åˆ†æ")
    print(f"=" * 60)
    print(f"æœºæˆ¿: {datacenter}")
    print(f"é˜²ç«å¢™IP: {host_ip}")
    print(f"åˆ†ææ—¶é—´çª—å£: {time_window}ç§’")
    print(f"æ€»é€’å¢åŒºé—´æ•°: {len(impact_intervals)}")
    
    if debug_mode:
        print(f"\nğŸ“Š å˜åŒ–å‰§çƒˆç¨‹åº¦å›¾ä¾‹:")
        print(f"  * = è½»å¾®(25%ä»¥ä¸‹)   ** = è¾ƒå°(25-50%)   *** = ä¸­ç­‰(50-100%)")
        print(f"  **** = è¾ƒå¤§(100-200%)   ***** = ä¸¥é‡(200-500%)   ****** = æç«¯(500%+)")
    
    # ç»Ÿè®¡æœ€ä¸¥é‡çš„å½±å“
    max_traffic_increase = 0
    max_response_degradation = 0
    critical_interval = None
    max_traffic_interval = None
    max_response_interval = None
    max_absolute_traffic = 0
    max_absolute_interval = None
    max_response_ms = 0
    max_response_ms_interval = None
    max_response_earlier_avg = 0
    max_response_recent_avg = 0
    
    # åªåœ¨debugæ¨¡å¼ä¸‹æ‰“å°æ¯ä¸ªåŒºé—´çš„è¯¦ç»†ä¿¡æ¯
    if debug_mode:
        for i, interval in enumerate(impact_intervals):
            business_impact = interval['business_impact']
            analysis = business_impact['analysis']
            
            # è·å–å˜åŒ–å‰§çƒˆç¨‹åº¦æ ‡è¯†ç¬¦
            intensity_indicator = get_change_intensity_indicator(interval['increase_percent'])
            
            print(f"\nğŸ“ˆ åŒºé—´ {i+1}: {interval['traffic_type']}æµé‡å¼‚å¸¸ {intensity_indicator}")
            print(f"  æ—¶é—´æ®µ: {interval['start_time'].strftime('%Y-%m-%d %H:%M:%S')} ~ {interval['end_time'].strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"  æŒç»­æ—¶é—´: {interval['duration_seconds']:.0f}ç§’")
            print(f"  æµé‡å˜åŒ–: {interval['start_formatted']} -> {interval['end_formatted']} (+{interval['increase_percent']:.1f}%)")
            print(f"  å³°å€¼æµé‡: {interval['peak_formatted']}")
            print(f"  æ•°æ®åº“å“åº”: å“åº”æ—¶é—´{analysis['trend']}, å¹³å‡{analysis['avg_ms']:.1f}ms, æœ€å¤§{analysis['max_ms']:.1f}ms")
    
    # æ— è®ºdebugæ¨¡å¼å¦‚ä½•ï¼Œéƒ½è¦ç»Ÿè®¡æœ€å¤§å€¼ç”¨äºåç»­åˆ†æ
    for i, interval in enumerate(impact_intervals):
        business_impact = interval['business_impact']
        analysis = business_impact['analysis']
        
        if 'max_ms' in analysis and analysis['max_ms'] > max_response_ms:
            max_response_ms = analysis['max_ms']
            max_response_ms_interval = interval
        
        # æ›´æ–°æœ€å¤§æµé‡å¢é•¿ç»Ÿè®¡
        if interval['increase_percent'] > max_traffic_increase:
            max_traffic_increase = interval['increase_percent']
            max_traffic_interval = interval
        
        # æ›´æ–°æœ€å¤§ç»å¯¹æµé‡ç»Ÿè®¡
        if interval['peak_value'] > max_absolute_traffic:
            max_absolute_traffic = interval['peak_value']
            max_absolute_interval = interval
        
        # è®¡ç®—å“åº”æ—¶é—´æ¶åŒ–ç¨‹åº¦
        if analysis['data_points'] >= 3:
            # ä»ä¸šåŠ¡å“åº”æ•°æ®ä¸­è·å–åŸå§‹æ•°æ®è¿›è¡Œè®¡ç®—
            response_data = business_impact.get('response_data', [])
            if not response_data:
                # å¦‚æœæ²¡æœ‰åŸå§‹æ•°æ®ï¼Œä½¿ç”¨mockæ•°æ®é‡æ–°ç”Ÿæˆæ¥è®¡ç®—
                response_data = get_business_response_data(
                    "mock_host", 
                    interval['start_time'], 
                    interval['end_time']
                ).get('database_response', [])
            
            if response_data and len(response_data) >= 3:
                values = [item['value'] for item in response_data]
                mid_point = len(values) // 2
                earlier_avg = sum(values[:mid_point]) / mid_point if mid_point > 0 else values[0]
                recent_avg = sum(values[mid_point:]) / (len(values) - mid_point)
                degradation_ratio = (recent_avg - earlier_avg) / earlier_avg if earlier_avg > 0 else 0
                
                if degradation_ratio > max_response_degradation:
                    max_response_degradation = degradation_ratio
                    max_response_interval = interval
                    critical_interval = interval
                    max_response_earlier_avg = earlier_avg
                    max_response_recent_avg = recent_avg
    
    # ç»¼åˆè¯„ä¼° - åˆ†ä¸‰ä¸ªç»´åº¦æ˜¾ç¤º
    print(f"\nğŸ“Š ç»¼åˆå½±å“è¯„ä¼°:")
    
    # åˆ†åˆ«ç»Ÿè®¡å…¥ç«™æµé‡ã€å‡ºç«™æµé‡
    inbound_intervals = [i for i in impact_intervals if i['traffic_type'] == 'inbound']
    outbound_intervals = [i for i in impact_intervals if i['traffic_type'] == 'outbound']
    
    # å…¥ç«™æµé‡åˆ†æ
    if inbound_intervals:
        max_inbound_increase = max(interval['increase_percent'] for interval in inbound_intervals)
        max_inbound_interval = max(inbound_intervals, key=lambda x: x['increase_percent'])
        max_inbound_peak_interval = max(inbound_intervals, key=lambda x: x['peak_value'])
        
        print(f"  ã€å…¥ç«™æµé‡ã€‘")
        print(f"    æœ€å¤§æµé‡å¢é•¿ç‡: {max_inbound_increase:.1f}% æµé‡å˜åŒ–: {max_inbound_interval['start_formatted']} -> {max_inbound_interval['end_formatted']} å‘ç”Ÿæ—¶é—´: {max_inbound_interval['start_time'].strftime('%Y-%m-%d %H:%M:%S')} ~ {max_inbound_interval['end_time'].strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"    æœ€å¤§æµé‡å€¼: {max_inbound_peak_interval['peak_formatted']} å‘ç”Ÿæ—¶é—´: {max_inbound_peak_interval['start_time'].strftime('%Y-%m-%d %H:%M:%S')} ~ {max_inbound_peak_interval['end_time'].strftime('%Y-%m-%d %H:%M:%S')}")
    else:
        print(f"  ã€å…¥ç«™æµé‡ã€‘æ— å¼‚å¸¸åŒºé—´")
    
    # å‡ºç«™æµé‡åˆ†æ
    if outbound_intervals:
        max_outbound_increase = max(interval['increase_percent'] for interval in outbound_intervals)
        max_outbound_interval = max(outbound_intervals, key=lambda x: x['increase_percent'])
        max_outbound_peak_interval = max(outbound_intervals, key=lambda x: x['peak_value'])
        
        print(f"  ã€å‡ºç«™æµé‡ã€‘")
        print(f"    æœ€å¤§æµé‡å¢é•¿ç‡: {max_outbound_increase:.1f}% æµé‡å˜åŒ–: {max_outbound_interval['start_formatted']} -> {max_outbound_interval['end_formatted']} å‘ç”Ÿæ—¶é—´: {max_outbound_interval['start_time'].strftime('%Y-%m-%d %H:%M:%S')} ~ {max_outbound_interval['end_time'].strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"    æœ€å¤§æµé‡å€¼: {max_outbound_peak_interval['peak_formatted']} å‘ç”Ÿæ—¶é—´: {max_outbound_peak_interval['start_time'].strftime('%Y-%m-%d %H:%M:%S')} ~ {max_outbound_peak_interval['end_time'].strftime('%Y-%m-%d %H:%M:%S')}")
    else:
        print(f"  ã€å‡ºç«™æµé‡ã€‘æ— å¼‚å¸¸åŒºé—´")
    
    # æ•°æ®åº“å“åº”è€—æ—¶åˆ†æ
    print(f"  ã€æ•°æ®åº“å“åº”è€—æ—¶ã€‘")
    if max_response_interval and max_response_degradation > 0:
        print(f"    æœ€å¤§å“åº”è€—æ—¶å¢é•¿ç‡: {max_response_degradation*100:.1f}% å“åº”å˜åŒ–:{max_response_earlier_avg:.1f}ms->{max_response_recent_avg:.1f}ms å‘ç”Ÿæ—¶é—´:{max_response_interval['start_time'].strftime('%Y-%m-%d %H:%M:%S')} ~ {max_response_interval['end_time'].strftime('%Y-%m-%d %H:%M:%S')}")
    else:
        print(f"    æœ€å¤§å“åº”è€—æ—¶å¢é•¿ç‡: æ— æ³•è®¡ç®—ï¼ˆæ•°æ®ä¸è¶³ï¼‰")
        
    if max_response_ms_interval:
        print(f"    æœ€å¤§å“åº”è€—æ—¶: {max_response_ms:.1f}ms å‘ç”Ÿæ—¶é—´:{max_response_ms_interval['start_time'].strftime('%Y-%m-%d %H:%M:%S')} ~ {max_response_ms_interval['end_time'].strftime('%Y-%m-%d %H:%M:%S')}")
    else:
        print(f"    æœ€å¤§å“åº”è€—æ—¶: æ— æ³•è®¡ç®—ï¼ˆæ•°æ®ä¸è¶³ï¼‰")
    
    # æ€»ä½“å½±å“çº§åˆ«è¯„ä¼°
    total_intervals = len(impact_intervals)
    severe_intervals = len([i for i in impact_intervals if i['increase_percent'] >= 500])  # ******çº§åˆ«
    critical_intervals = len([i for i in impact_intervals if i['increase_percent'] >= 200])  # *****çº§åˆ«ä»¥ä¸Š
    
    # print(f"  å½±å“åŒºé—´ç»Ÿè®¡: æ€»è®¡{total_intervals}ä¸ª, ä¸¥é‡{critical_intervals}ä¸ª, æç«¯{severe_intervals}ä¸ª")
    
    # å½±å“çº§åˆ«åˆ¤å®š
    if severe_intervals >= 3:
        impact_level = "ç´§æ€¥"
    elif critical_intervals >= 5:
        impact_level = "ä¸¥é‡"
    elif total_intervals >= 10:
        impact_level = "ä¸­ç­‰"
    else:
        impact_level = "è½»å¾®"
    
    # print(f"  ç»¼åˆå½±å“çº§åˆ«: {impact_level}")
    
    # æ—¶é—´åˆ†å¸ƒåˆ†æ
    # if len(impact_intervals) >= 2:
    #     first_time = min(interval['start_time'] for interval in impact_intervals)
    #     last_time = max(interval['end_time'] for interval in impact_intervals)
    #     duration_minutes = (last_time - first_time).total_seconds() / 60
    #     print(f"  å½±å“æ—¶é—´è·¨åº¦: {first_time.strftime('%H:%M:%S')} ~ {last_time.strftime('%H:%M:%S')} (å…±{duration_minutes:.1f}åˆ†é’Ÿ)")
    
    # ç”ŸæˆAIåˆ†æ
    if critical_interval:
        prompt = generate_interval_analysis_prompt(datacenter, host_ip, time_window, critical_interval, impact_intervals)
        
        log_message("æ­£åœ¨è¿›è¡ŒAIåŒºé—´å½±å“åˆ†æ...")
        ai_analysis = call_deepseek_api(prompt)
        
        if ai_analysis:
            print(f"\nğŸ¤– AIåˆ†æç»“æœ:")
            print("=" * 50)
            print(ai_analysis)
        else:
            print(f"\nâš ï¸ ç»“è®º: æ£€æµ‹åˆ°{len(impact_intervals)}ä¸ªæµé‡é€’å¢åŒºé—´å¯¹ä¸šåŠ¡é€ æˆå½±å“")
    else:
        print(f"\nâš ï¸ ç»“è®º: æ£€æµ‹åˆ°{len(impact_intervals)}ä¸ªæµé‡é€’å¢åŒºé—´å¯¹ä¸šåŠ¡é€ æˆå½±å“")

def get_business_response_data(host_ip: str, time_from: datetime, time_till: datetime) -> Dict[str, List]:
    """è·å–ä¸šåŠ¡å“åº”æ•°æ® - ç›®å‰ä½¿ç”¨mockæ•°æ®"""
    log_message("è·å–æ•°æ®åº“å“åº”æ•°æ® (ä½¿ç”¨mockæ•°æ®)...")
    
    # Mockæ•°æ®ç”Ÿæˆ
    import random
    
    # è®¡ç®—æ—¶é—´ç‚¹æ•°é‡
    duration_seconds = int((time_till - time_from).total_seconds())
    data_points = max(3, duration_seconds // 30)  # æ¯30ç§’ä¸€ä¸ªæ•°æ®ç‚¹ï¼Œæœ€å°‘3ä¸ªç‚¹
    
    # ç”Ÿæˆmockæ•°æ®åº“å“åº”æ•°æ®
    base_db_time = 15  # åŸºç¡€æ•°æ®åº“å“åº”æ—¶é—´ 15ms
    db_response_data = []
    
    for i in range(data_points):
        # æ ¹æ®æ—¶é—´è¿›åº¦è®¡ç®—å“åº”æ—¶é—´å˜åŒ–
        time_point = time_from + timedelta(seconds=i * 30)
        
        # æ¨¡æ‹Ÿæ•°æ®åº“å“åº”æ—¶é—´è¶‹åŠ¿
        if random.random() < 0.3:  # 30%æ¦‚ç‡æ¨¡æ‹Ÿå“åº”æ—¶é—´æ¶åŒ–
            # æ¨¡æ‹Ÿå“åº”æ—¶é—´é€æ¸æ¶åŒ–
            degradation_factor = 1 + (i / data_points) * 3  # æœ€å¤šæ¶åŒ–åˆ°4å€
            db_time = base_db_time * degradation_factor + random.uniform(-5, 15)
        else:
            # æ¨¡æ‹Ÿæ­£å¸¸å“åº”æ—¶é—´
            db_time = base_db_time + random.uniform(-8, 20)
        
        db_response_data.append({
            'time': time_point,
            'value': max(1, db_time)  # æ•°æ®åº“å“åº”æ—¶é—´æœ€å°‘1ms
        })
    
    mock_data = {
        "database_response": db_response_data
    }
    
    log_message(f"Mockæ•°æ®åº“å“åº”æ•°æ®ç”Ÿæˆå®Œæˆ: {len(db_response_data)}ä¸ªç‚¹")
    
    return mock_data

def analyze_business_response_for_interval(host_ip: str, interval: Dict) -> Dict:
    """åˆ†æç‰¹å®šæ—¶é—´åŒºé—´çš„ä¸šåŠ¡å“åº”"""
    log_message(f"åˆ†æé€’å¢åŒºé—´æ•°æ®åº“å“åº”: {interval['start_time'].strftime('%Y-%m-%d %H:%M:%S')} ~ {interval['end_time'].strftime('%Y-%m-%d %H:%M:%S')}")
    
    # è·å–å¯¹åº”æ—¶é—´æ®µçš„æ•°æ®åº“å“åº”æ•°æ®
    business_data = get_business_response_data(host_ip, interval['start_time'], interval['end_time'])
    
    if not business_data or 'database_response' not in business_data:
        return {
            "has_data": False,
            "impact_detected": False,
            "analysis": None,
            "response_data": []
        }
    
    # åˆ†æè¿™ä¸ªæ—¶é—´æ®µçš„æ•°æ®åº“å“åº”
    response_analysis = analyze_response_trend(business_data['database_response'])
    
    # åˆ¤æ–­æ˜¯å¦æœ‰ä¸šåŠ¡å½±å“
    impact_detected = response_analysis['is_degrading']
    
    return {
        "has_data": True,
        "impact_detected": impact_detected,
        "analysis": response_analysis,
        "response_data": business_data['database_response'],  # ä¿å­˜åŸå§‹æ•°æ®
        "interval_info": {
            "duration": interval['duration_seconds'],
            "traffic_increase": interval['increase_percent'],
            "start_traffic": interval['start_formatted'],
            "end_traffic": interval['end_formatted'],
            "peak_traffic": interval['peak_formatted']
        }
    }

def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥å‚æ•°
    if len(sys.argv) < 4 or len(sys.argv) > 5:
        print("""ğŸ”¥ é˜²ç«å¢™æµé‡ä¸ä¸šåŠ¡å“åº”å…³è”åˆ†æè„šæœ¬

ç”¨æ³•:
  python firewall_simple_monitor.py <æœºæˆ¿> <IP> <æ—¶é—´çª—å£ç§’æ•°> [debug=on|off]
  
ç¤ºä¾‹:
  python firewall_simple_monitor.py IDC1 192.168.1.1 300
  python firewall_simple_monitor.py IDC1 192.168.1.1 300 debug=on
  python firewall_simple_monitor.py IDC2 10.0.0.1 600 debug=off
  
è¯´æ˜:
  - ä»å½“å‰æ—¶é—´å¾€å‰æ¨æŒ‡å®šç§’æ•°è·å–æ•°æ®
  - æ£€æµ‹æµé‡æ˜¯å¦æŒç»­å‡é«˜
  - å¦‚æœæµé‡æŒç»­å‡é«˜ï¼Œåˆ†æå¯¹ä¸šåŠ¡å“åº”çš„å½±å“
  - debug=on: æ˜¾ç¤ºæ¯ä¸ªåŒºé—´çš„è¯¦ç»†ä¿¡æ¯
  - debug=off: åªæ˜¾ç¤ºç»¼åˆåˆ†æç»“æœ (é»˜è®¤)""")
        sys.exit(1)
    
    datacenter = sys.argv[1]
    host_ip = sys.argv[2]
    
    try:
        time_window = int(sys.argv[3])
    except ValueError:
        print("é”™è¯¯ï¼šæ—¶é—´çª—å£å¿…é¡»æ˜¯æ•°å­—ï¼ˆç§’ï¼‰")
        sys.exit(1)
    
    # è§£ædebugå‚æ•°
    debug_mode = False
    if len(sys.argv) == 5:
        debug_param = sys.argv[4].lower()
        if debug_param == "debug=on":
            debug_mode = True
        elif debug_param == "debug=off":
            debug_mode = False
        else:
            print("é”™è¯¯ï¼šdebugå‚æ•°æ ¼å¼åº”ä¸º debug=on æˆ– debug=off")
            sys.exit(1)
    
    # è®¡ç®—æ—¶é—´èŒƒå›´
    time_till = datetime.now()
    time_from = time_till - timedelta(seconds=time_window)
    
    log_message(f"å¼€å§‹åˆ†æ - æœºæˆ¿:{datacenter}, IP:{host_ip}, æ—¶é—´çª—å£:{time_window}ç§’")
    log_message(f"åˆ†ææ—¶é—´æ®µ: {time_from.strftime('%Y-%m-%d %H:%M:%S')} ~ {time_till.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # è·å–æµé‡æ•°æ®
    log_message("è·å–é˜²ç«å¢™æµé‡æ•°æ®...")
    traffic_data = get_metrics_data(host_ip, TRAFFIC_METRICS, time_from, time_till)
    
    if not traffic_data:
        log_message("æµé‡æ•°æ®è·å–å¤±è´¥", "ERROR")
        sys.exit(1)
    
    # åˆ†æå…¥æµé‡å’Œå‡ºæµé‡è¶‹åŠ¿
    inbound_analysis = analyze_traffic_with_intervals(traffic_data.get('inbound_traffic', []))
    outbound_analysis = analyze_traffic_with_intervals(traffic_data.get('outbound_traffic', []))
    
    log_message(f"å…¥æµé‡: {inbound_analysis['trend']}, æ•°æ®ç‚¹:{inbound_analysis['data_points']}, æœ€å¤§:{inbound_analysis['max_formatted']}")
    log_message(f"å‡ºæµé‡: {outbound_analysis['trend']}, æ•°æ®ç‚¹:{outbound_analysis['data_points']}, æœ€å¤§:{outbound_analysis['max_formatted']}")
    
    # æ”¶é›†æ‰€æœ‰é€’å¢åŒºé—´
    all_increase_intervals = []
    
    # æ·»åŠ å…¥æµé‡é€’å¢åŒºé—´
    for interval in inbound_analysis['increase_intervals']:
        interval['traffic_type'] = 'inbound'
        all_increase_intervals.append(interval)
    
    # æ·»åŠ å‡ºæµé‡é€’å¢åŒºé—´
    for interval in outbound_analysis['increase_intervals']:
        interval['traffic_type'] = 'outbound'
        all_increase_intervals.append(interval)
    
    if not all_increase_intervals:
        log_message("æœªæ£€æµ‹åˆ°æµé‡é€’å¢åŒºé—´ï¼Œæ— éœ€æ£€æŸ¥ä¸šåŠ¡å“åº”")
        print(f"ç»“è®ºï¼šæœºæˆ¿{datacenter}é˜²ç«å¢™{host_ip}æµé‡æ­£å¸¸ï¼Œæ— ä¸šåŠ¡å½±å“")
        sys.exit(0)
    
    log_message(f"æ£€æµ‹åˆ°{len(all_increase_intervals)}ä¸ªæµé‡é€’å¢åŒºé—´ï¼Œå¼€å§‹é€ä¸€åˆ†æä¸šåŠ¡å½±å“...")
    
    # åˆ†ææ¯ä¸ªé€’å¢åŒºé—´çš„ä¸šåŠ¡å½±å“
    impact_intervals = []
    
    for i, interval in enumerate(all_increase_intervals):
        intensity_indicator = get_change_intensity_indicator(interval['increase_percent'])
        log_message(f"åˆ†æç¬¬{i+1}ä¸ªé€’å¢åŒºé—´ ({interval['traffic_type']}æµé‡) {intensity_indicator}: {interval['start_formatted']} -> {interval['end_formatted']} (+{interval['increase_percent']:.1f}%)")
        
        # åˆ†æè¿™ä¸ªåŒºé—´çš„ä¸šåŠ¡å“åº”
        interval_analysis = analyze_business_response_for_interval(host_ip, interval)
        
        if interval_analysis['has_data']:
            if interval_analysis['impact_detected']:
                log_message(f"ç¬¬{i+1}ä¸ªåŒºé—´æ£€æµ‹åˆ°ä¸šåŠ¡å½±å“: å“åº”æ—¶é—´{interval_analysis['analysis']['trend']}")
                interval['business_impact'] = interval_analysis
                impact_intervals.append(interval)
            else:
                log_message(f"ç¬¬{i+1}ä¸ªåŒºé—´ä¸šåŠ¡å“åº”æ­£å¸¸: å¹³å‡{interval_analysis['analysis']['avg_ms']:.1f}ms")
        else:
            log_message(f"ç¬¬{i+1}ä¸ªåŒºé—´æ— æ³•è·å–ä¸šåŠ¡å“åº”æ•°æ®")
    
    if not impact_intervals:
        log_message("æ‰€æœ‰é€’å¢åŒºé—´éƒ½æœªå¯¹ä¸šåŠ¡é€ æˆå½±å“")
        print(f"ç»“è®ºï¼šæœºæˆ¿{datacenter}é˜²ç«å¢™{host_ip}è™½æœ‰æµé‡é€’å¢ï¼Œä½†æ— ä¸šåŠ¡å½±å“")
        sys.exit(0)
    
    log_message(f"å‘ç°{len(impact_intervals)}ä¸ªé€’å¢åŒºé—´å¯¹ä¸šåŠ¡é€ æˆå½±å“ï¼Œç”Ÿæˆåˆ†ææŠ¥å‘Š...")
    
    # ç”Ÿæˆå—å½±å“åŒºé—´çš„ç»¼åˆåˆ†æ
    generate_interval_impact_report(datacenter, host_ip, time_window, impact_intervals, debug_mode)

if __name__ == "__main__":
    main() 